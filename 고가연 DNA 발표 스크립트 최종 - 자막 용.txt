@는 애니메이션임


1) 안녕하십니까 수원대학교 데이터 과학부 19학번 고가연입니다.
주가와 검색어 간의 상관관계를 "주가의 하루 평균치와 키워드 검색어량"을 통해 분석해보도록 하겠습니다.

2) 목차는 다음과 같습니다.

3) 발표에 앞서 간단히 제 소개부터 하겠습니다.
어려서부터 돈 모으는 것을 좋아하던 저는 드디어 작년에 그동안 모아둔 목돈의 1/3로 주식을 시작했습니다. 작년 한 해 저의 최대 관심사는 이번 프로젝트의 주제를 잡는 데에 큰 영향을 미쳤습니다.

작년 하반기부터는 갑자기 혼자 살게 되면서 "삶의 질 향상"에 대해 관심을 갖게 됐습니다.
어떻게 하면 시간을 적게 들여서 삶의 질을 최대한으로 끌어올릴 수 있는지에 대해 생각하고 있습니다.
결국 답은 돈 밖에 없어서 정말 혼자가 되기 전에 돈을 많이 모아 놔야겠다고 또 생각하게 됐습니다.

개강이 다가오면서, 올해는 작년보다 열심히 살겠다고 다짐하고 있는 19학번 고가연의 자기소개였습니다.

다시 발표로 돌아가서.

4) 프로젝트의 주제는 큰 틀로 주식과 관련된 걸 하고싶다고만 생각을 하다가
증권사에 종사하시는 삼촌과 대화를 나누게 되면서 좀 더 자세히 주제를 잡을 수 있었습니다.
프로젝트 준비 초기엔 실검과 주가와의 상관관계를 분석해보려고 했으나, 장기간 상세한 갈피를 잡지 못해 선배와의 상담을 통해 주제를 살짝 틀어 검색어량과 주가간의 상관관계를 분석하는 주식데이터 프로젝트를 찾아 따라해보기로 했습니다.
(제가 따라한 프로젝트는 ppt 하단에 올려두었습니다.)

5) 전체적인 흐름은 다음과 같습니다.
분석에 사용할 기업을 선정하고,
selenium 패키지로 네이버에서 해당 기업을 쿼리 (검색어)로 해서 기사들을 크롤링합니다.
형태소 분석기 중 twitter(트위터(Okt))를 이용해 기사에서 키워드를 추출해서 그 중 상위 3개의 검색어량과 야후 파이낸스 패키지로 크롤링한 해당 기업의 주가 사이의 상관관계를 분석합니다.

6) 제일 먼저 기업과 기간을 선정한 과정을 말씀드리겠습니다.
우선 분석에 사용할 기업은 주식하는 한국인이라면 누구나 한 주 쯤은 품고 있을 "삼성전자"로 선택했고,
기간은 주가가 뉴스에 반응할 것이라는 가정을 갖고 시작했으므로 새 소식의 키워드에 주가가 바로 매치될 수 있도록 짧게 잡았습니다.

7) 다음으로 기사에서 키워드를 추출하는 과정을 알아보겠습니다.
query를 삼성전자로 해서 최근 일주일동안의 기사를 100개 크롤링해옵니다.
이 때, 기사 본문은 text열에 저장합니다.
크롤링한 기사는 엑셀파일로 저장합니다.
형태소 분석을 한번에 하기 위해 text열을 하나의 문자열로 만든 후, 형태소 분석기 Twitter(Okt)를 사용해 키워드를 추출했습니다.

8) 형태소 분석기로 Twitter(트위터(Okt))를 사용한 이유는 Twitter(트위터(Okt))가 사용하기에 제일 무난하다고 판단했기 때문입니다.
Kkma(꼬꼬마), Mecab에 대한 리뷰를 찾아본 결과 꼬꼬마는 시간이 오래걸리고, Mecab은 Twitter(Okt)보다 빠르나 잘게 나눈다는 특징이 있었습니다.
사실 제가 하는 작업이 분석 속도가 딱히 상관이 없는 작업이긴 했지만, 그래도 느린 것보다 빠른 게 낫다고 판단했기 때문에 Kkma(꼬꼬마)는 제외했습니다. 단어를 더 잘게 나누는 것에 대해서도 신조어 같은 분석기가 파악하지 못할 단어가 나올 수도 있어 사용하지 않기로 했습니다.
이 외에도 박현기 선배님의 도움으로 Komoran(코모란), Hannanum(한나눔)을 분석을 마친 후 알게 됐는데, 다음에 형태소 분석기를 사용할 일이 오게 된다면 Komoran(코모란)을 사용할 것 같습니다.

선배님께서 제공해주신 분석기 비교자료를 통해, Komoran(코모란)이 띄어쓰기가 제대로 되어있지 않은 문장도 잘 분석해내고, 오탈자가 있는 문장은 오탈자를 수정해서까지 분석해내고, 거기에 비록 Mecab에는 한참 뒤지지만 속도면에서도 우수한 편에 속하는 것을 확인했기 때문입니다.
(선배님께서 제공해주신 비교 자료에 의하면 Mecab은 속도 외에는 매력이 없었습니다.)
오탈자가 있는 문장이나 띄어쓰기가 제대로 되어있지 않은 문장에서는 분석력이 좋다고 할 수 없었습니다.)


9) 다음 다섯(5) 페이지는 앞에서 설명한 기사를 크롤링 해와서 키워드를 추출하는 과정의 코드에 대한 보다 자세한 설명인데 큼직한 꼬리만 설명하고 넘어가도록 하겠습니다.

10) 첫 번째로 언론사 별로 본문이 존재하는 태그가 다 달랐기 때문에 일일이 태그를 찾아 파싱 함수를 만들어줬습니다.
따라서 언론사도 일부만 선택해서 사용했습니다. 나중에 확인해보시면 아시겠지만 따라한다고 했던 블로그가 선택한 언론사 중 "매일경제"는 제외했습니다. "매일경제"는 기사 카테고리 별로 여러 사이트가 존재했고, 사이트 별로 기사 본문의 위치가 달라 에러를 자꾸 발생시켰는데, 마지막에 "칼럼"사이트가 발생시키는 에러를 끝내 해결하지 못했기 때문입니다.

11) 두 번째로 query라는 변수에 기업명을 입력해 브라우저를 열어 검색해줍니다. 좀 더 쉽게 가기 위해 입력한 검색어를 검색해주는 query가 포함된 주소를 이용했습니다.
다음으로 검색옵션을 선택합니다. 기간과 기사를 따올 언론사를 이 단계에서 설정해야 합니다.

12) 세 번째로 드디어 뉴스를 크롤링합니다. 뉴스는 제목, 주소, 본문을 따와서 dictionary 형태로 저장해줍니다.

13) 네 번째로 dictionary 형태로 저장된 뉴스 목록을 dataframe으로 만들어 주고, excel로 저장해줍니다.
다음으로 excel 파일을 불러와서 기사 본문에서 키워드를 추출해주고, 워드클라우드를 봐가면서 일일이 불용어 처리를 해주었습니다. 불용어는 훨씬 많았지만 ppt에는 일부만 넣었습니다.

14) 다섯 번째로 워드클라우드를 생성해주는 코드입니다.

15) 다음은 키워드의 검색량을 추출하는 과정입니다.
네이버 데이터랩의 검색어트렌드 open api를 이용했습니다.

16) 검색어 트렌드 api를 이용하기 위해서 애플리케이션 등록이 필요한데, 다음 17페이지까지 이어지는 방법을 따라하시면 됩니다.
이 과정은 제공되는 ppt를 보시고 따라해주시면 감사하겠습니다.

17) 등록하면 바로 아이디와 비밀번호가 제공되며, 서비스를 이용할 수 있습니다.

18) API에 대한 설명은 Documents 배너에서 볼 수 있습니다.
해당 페이지에서는 사용자(개발자)가 이용할 수 있는 코드를 제공해주기 때문에 이용하실 때 꼭 확인해보시는 것이 좋습니다.

19) 다음 페이지는 각 키워드의 검색량을 추출하는 코드입니다.

20) 키워드는 기사 본문에서 추출한 키워드 중 상위에 있는 3개를 사용했으며, 각각 "취업", "공장", "갤럭시"를 사용했습니다.
삼성전자 기사면 "부회장"이 키워드로 많이 나왔을텐데, 이를 메인 키워드로 사용하지 않은 이유는 곧 말씀드리겠습니다.

파라미터 keywordGroups에는 최대 5쌍까지 검색어 묶음을 설정할 수 있어서, 키워드 3개를 각각의 묶음으로 만들어 넣어줬습니다. 이를 하나의 묶음으로 만들 경우 각각의 쿼리양이 나오는 게 아니라 평균치가 나오기 때문에 각각의 쿼리양이 필요하다면 꼭 따로 묶어서 넣어줘야합니다.

키워드 "취업"은 요즘 이재용 부회장의 취업제한과 관련된 키워드임을 확인하고, 그룹명을 "이재용 취업 제한"으로 짓고 "(이재용) 취업 제한", "이재용", "(삼성전자) 부회장" 등을 묶어줬습니다.
"부회장"을 따로 키워드에 넣어주지 않은 이유가 여기에 있습니다. 최근 일주일 동안의 기사에서 "이재용 부회장"은 거의 "취업제한"과 함께 언급됐었기 때문에 굳이 따로 넣어주지 않고 "취업제한"과 묶어서 넣어줬습니다.

키워드 "공장"과는 네이버에 삼성전자 공장을 쳐서 나온 연관검색어 및 기사와 상위 키워드를 보고 연관이 있을법한 "(삼성전자) 반도체"와 "(삼성전자) 미국", "(삼성전자) 오스틴" 등을 묶어줬습니다.

마지막으로 키워드 "갤럭시"와는 "(삼성전자) 스마트폰", "(삼성전자) 모바일 기기" 등을 묶어줬습니다.

키워드 묶음을 다 만든 후에는 네이버 개발자센터에서 제공해주는 코드를 이용해 쿼리양을 뽑고,
뒤에서 사용하기 위해 뽑은 쿼리양 각각으로 dataframe을 만들어 줍니다.

21) 다음은 주가 차트를 불러오는 과정입니다. 야후 파이낸스 패키지를 이용하면 기업 코드만 알면 되기 때문에 보다 @@편하게 가져올 수 있어서 해당 패키지를 사용해 주가차트를 가져왔습니다.

22) 다음은 주식 차트를 불러와 검색량 dataframe과 결합하고 필요한 열만 남기는 코드입니다.

23) 일단, 앞 (21) 페이지에서 보여드린 코드로 주식 표를 가져와 변수(data)에 저장해줍니다.
그 다음 데이터 프레임(data)에 존재하는 high와 low열을 가지고 평균을 내서 data옆에 붙여줍니다.
앞선 "과정 2"에서 만들어놓은 데이터 프레임들을 합쳐서 data 왼쪽으로 붙여준 후, 필요한 열만 선택해서 남겨줍니다. 아래는 비어있는 cell이 있다면 채워주는 코드입니다.

거의 다 왔습니다.
24) 다음은 상관관계를 시각화 하는 과정입니다. 상관관계를 시각화 하기에 앞서 각 키워드들의 검색량과 주가 라인차트를 하나의 axes 위에 그려보았습니다.

@다음으로 각 키워드들의 검색량과 주가와의 상관관계를 출력한 결과입니다.
초기에 설정한 키워드 3개 중 "갤럭시"가 그나마 삼성전자 주가와 상관관계가 있다고 볼 수 있다는 결과가 나왔습니다.
(하지만 이 결과에는 오류가 있을 수 있습니다. 왜냐하면 검색어량 상승은 주가 하락과 함께 했을 수도 있기 때문입니다.)

25) 다음은 시각화 코드입니다.
시각화 코드 중 삼각형 마스크를 생성하고 적용하는 코드만 짚고 넘어가겠습니다.
triu는 대각성분 위쪽 삼각형(upper triangle)을 의미해서 다음과 같이 코드를 치면 대각행렬 위쪽만 true가 됩니다.
이를 heatmap의 mask 파라미터에 넣어주면 true로 표시한 부분이 나타나지 않게 돼 왼쪽의 그림처럼 상관관계 히트맵을 한쪽만 나타낼 수 있습니다.

26) 인상깊었던 에러에 대해 이야기 하는 것으로 마무리하겠습니다.
짧은 시간동안 크롤링을 좀 많이 하면 네이버에서 자꾸 제한을 걸어서
아래 보이는 에러가 계속 났습니다.
해결 방법을 찾았는데, 이는 아래 링크로 걸어두었습니다.
네이버가 안전하다는 것을 새삼 느낄 수 있었던 프로젝트였습니다.

27) 본 프로젝트의 결과는 아무래도 주식 시장에서는 크게 유효하지 않을 것 같으니, 결과에 크게 의미를 두지 마시길 바랍니다. 끝까지 들어주셔서 감사합니다.